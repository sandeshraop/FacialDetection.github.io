#!/usr/bin/env python3

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import yaml
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix, recall_score
from sklearn.model_selection import LeaveOneGroupOut
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm
import argparse
from pathlib import Path
import time
import math
import random

from data_loader.advanced_casme_loader import get_advanced_data_loaders, advanced_collate_fn
from models.lightweight_cnn import create_lightweight_model

def calculate_uar(y_true, y_pred):
    """Calculate Unweighted Average Recall (UAR) - CASME-II Standard Metric"""
    return recall_score(y_true, y_pred, average='macro') * 100

class CosineAnnealingWarmupRestarts(optim.lr_scheduler._LRScheduler):
    """Cosine annealing with warmup and restarts"""
    def __init__(self, optimizer, T_0, T_mult=1, eta_min=0, last_epoch=-1, warmup_epochs=0):
        self.T_0 = T_0
        self.T_mult = T_mult
        self.eta_min = eta_min
        self.warmup_epochs = warmup_epochs
        self.T_cur = 0
        self.last_epoch = last_epoch
        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)
        
    def get_lr(self):
        if self.last_epoch < self.warmup_epochs:
            # Warmup phase
            return [base_lr * (self.last_epoch + 1) / self.warmup_epochs for base_lr in self.base_lrs]
        else:
            # Cosine annealing phase
            if self.T_cur == 0:
                self.T_cur = self.last_epoch - self.warmup_epochs
            
            return [self.eta_min + (base_lr - self.eta_min) * 
                   (1 + math.cos(math.pi * self.T_cur / self.T_0)) / 2 
                   for base_lr in self.base_lrs]
    
    def step(self, epoch=None):
        if epoch is None:
            epoch = self.last_epoch + 1
            self.last_epoch = epoch
        else:
            self.last_epoch = epoch
            
        if self.last_epoch >= self.warmup_epochs:
            self.T_cur = (self.last_epoch - self.warmup_epochs) % self.T_0
            if self.T_cur == 0 and self.last_epoch > self.warmup_epochs:
                self.T_0 = self.T_0 * self.T_mult
                
        self.last_epoch = epoch
        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):
            param_group['lr'] = lr

def train_single_subject_fold_optical_flow(subject_id, config, device):
    """ðŸš€ Optical Flow training for a single subject - CASME-II SOTA approach"""
    print(f'\n=== Optical Flow Training Subject {subject_id} ===')
    print(f'ðŸ”¥ Using motion (Onsetâ†’Apex) instead of RGB frames')
    
    # Create data loaders with LOSO
    train_loader, val_loader, test_loader, class_weights = get_advanced_data_loaders(config['data'], test_subject_id=subject_id)
    
    # Create lightweight model (appropriate for ~250 samples)
    model = create_lightweight_model(num_classes=config['model']['num_classes'])
    model.to(device)
    
    # âŒ REMOVED: Mixup/CutMix (invalid for micro-expressions)
    # Micro-expressions depend on local muscle activation
    # Mixing faces creates non-existent facial actions
    
    # âŒ REMOVED: Complex differential learning rates (not needed for lightweight model)
    # Use simple optimizer for stability with small dataset
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['training']['lr'],
        weight_decay=float(config['training']['weight_decay'])
    )
    
    # Advanced scheduler
    scheduler = CosineAnnealingWarmupRestarts(
        optimizer,
        T_0=config['scheduler']['T_max'],
        eta_min=float(config['scheduler']['eta_min']),
        warmup_epochs=config['training']['warmup_epochs']
    )
    
    # Loss function - âœ… CASME-II Standard: Class-weighted CrossEntropy ONLY
    # âŒ REMOVED: Focal Loss (exaggerates minority noise in LOSO)
    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    
    # Mixed precision
    scaler = torch.amp.GradScaler('cuda') if device.type == 'cuda' else None
    
    # Training loop
    best_val_acc = 0.0
    patience_counter = 0
    max_patience = config['training']['early_stop']
    
    for epoch in range(config['training']['num_epochs']):
        # âŒ REMOVED: Gradual unfreezing (not needed for lightweight model)
        # Lightweight model trains end-to-end effectively
        
        # Training
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0
        
        train_pbar = tqdm(train_loader, desc=f'Subject {subject_id} Epoch {epoch+1}')
        for batch_idx, batch in enumerate(train_pbar):
            spatial_images = batch['spatial_image'].to(device)
            # temporal_sequences = batch['temporal_sequence'].to(device)  # âŒ Not used in lightweight CNN
            labels = batch['label'].to(device)
            
            # âŒ REMOVED: Mixup/CutMix augmentation (invalid for micro-expressions)
            
            optimizer.zero_grad()
            
            if scaler:
                with torch.amp.autocast('cuda'):
                    outputs = model(spatial_images)  # âœ… Simple CNN forward pass
                    loss = criterion(outputs, labels)  # âœ… Simple loss (no Mixup/CutMix)
                
                scaler.scale(loss).backward()
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(spatial_images)  # âœ… Simple CNN forward pass
                loss = criterion(outputs, labels)  # âœ… Simple loss (no Mixup/CutMix)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
            
            train_loss += loss.item()
            
            # âœ… Simple accuracy calculation (no Mixup/CutMix complications)
            _, predicted = torch.max(outputs.data, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            
            current_acc = 100 * train_correct / max(train_total, 1)
            train_pbar.set_postfix({'Loss': f'{train_loss/(batch_idx+1):.4f}', 'Acc': f'{current_acc:.2f}%'})
        
        # Validation
        model.eval()
        val_y_true, val_y_pred = [], []
        val_loss = 0.0  # Initialize val_loss
        
        with torch.no_grad():
            for batch in val_loader:
                spatial_images = batch['spatial_image'].to(device)
                # temporal_sequences = batch['temporal_sequence'].to(device)  # âŒ Not used in lightweight CNN
                labels = batch['label'].to(device)
                
                if scaler:
                    with torch.amp.autocast('cuda'):
                        outputs = model(spatial_images)  # âœ… Simple CNN forward pass
                        loss = criterion(outputs, labels)
                else:
                    outputs = model(spatial_images)  # âœ… Simple CNN forward pass
                    loss = criterion(outputs, labels)
                
                val_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                val_y_true.extend(labels.cpu().numpy())
                val_y_pred.extend(predicted.cpu().numpy())
        
        # Calculate UAR instead of accuracy
        val_uar = calculate_uar(val_y_true, val_y_pred)
        train_acc = 100 * train_correct / max(train_total, 1)  # Keep for training monitoring only
        
        scheduler.step()
        
        if val_uar > best_val_acc:
            best_val_acc = val_uar
            patience_counter = 0
            # Save best model
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'best_val_acc': best_val_acc,
                'subject_id': subject_id
            }, f'checkpoints/hybrid_subject_{subject_id}_best.pth')
        else:
            patience_counter += 1
        
        print(f'Subject {subject_id} Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val UAR: {val_uar:.2f}% (Best: {best_val_acc:.2f}%)')
        
        if patience_counter >= max_patience:
            print(f'Early stopping triggered after {epoch+1} epochs')
            break
    
    # Load best model for test evaluation
    checkpoint = torch.load(f'checkpoints/hybrid_subject_{subject_id}_best.pth')
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # Test evaluation with UAR
    model.eval()
    test_y_true, test_y_pred = [], []
    
    with torch.no_grad():
        for batch in test_loader:
            spatial_images = batch['spatial_image'].to(device)
            # temporal_sequences = batch['temporal_sequence'].to(device)  # âŒ Not used in lightweight CNN
            labels = batch['label'].to(device)
            
            outputs = model(spatial_images)  # âœ… Simple CNN forward pass
            _, predicted = torch.max(outputs, 1)
            test_y_true.extend(labels.cpu().numpy())
            test_y_pred.extend(predicted.cpu().numpy())
    
    # Calculate UAR instead of accuracy
    test_uar = calculate_uar(test_y_true, test_y_pred)
    
    print(f'âœ… Subject {subject_id} Optical Flow Results:')
    print(f'   Best Val UAR: {best_val_acc:.2f}%')
    print(f'   Test UAR: {test_uar:.2f}%')
    print(f'ðŸ”¥ Motion-based recognition with optical flow!')
    
    return {
        'subject_id': subject_id,
        'best_val_uar': best_val_acc,
        'test_uar': test_uar,
        'train_samples': len(train_loader.dataset),
        'test_samples': len(test_loader.dataset)
    }

def optical_flow_losos_demo(config, device):
    """ðŸš€ Optical Flow LOSO demo with motion-based recognition"""
    print('ðŸ”¥ Starting Optical Flow LOSO Demo - Motion-based Micro-Expression Recognition...')
    print('ðŸ“ˆ Expected UAR improvement: +8-15% over RGB baseline')
    
    # Test on subjects with sufficient data (>=10 samples based on analysis)
    # Good candidates: sub02(13), sub05(19), sub09(14), sub10(14), sub11(10), sub12(12), 
    # sub17(36), sub19(16), sub20(11), sub23(12), sub24(10), sub26(17)
    test_subjects = [5, 9, 12, 17, 19, 26]  # Selected diverse subjects with good data
    results = []
    
    for subject_id in test_subjects:
        try:
            result = train_single_subject_fold_optical_flow(subject_id, config, device)
            results.append(result)
        except Exception as e:
            print(f'âŒ Subject {subject_id} failed: {e}')
            continue
    
    # Analyze results
    if results:
        test_uars = [r['test_uar'] for r in results]
        val_uars = [r['best_val_uar'] for r in results]
        
        mean_test_uar = np.mean(test_uars)
        std_test_uar = np.std(test_uars)
        mean_val_uar = np.mean(val_uars)
        
        print(f'\nðŸ“Š Optical Flow LOSO Results ({len(results)} subjects):')
        print(f'   Mean Test UAR: {mean_test_uar:.2f}% Â± {std_test_uar:.2f}%')
        print(f'   Mean Validation UAR: {mean_val_uar:.2f}%')
        print(f'   Best Test UAR: {np.max(test_uars):.2f}%')
        print(f'   Worst Test UAR: {np.min(test_uars):.2f}%')
        print(f'ðŸ”¥ Motion-based recognition with optical flow!')
        
        print(f'\nðŸ“ˆ Per-Subject Results:')
        for result in results:
            print(f'   Subject {result["subject_id"]:02d}: Val={result["best_val_uar"]:.2f}%, Test={result["test_uar"]:.2f}%')
        
        # Performance analysis
        rgb_baseline = 12.82  # Previous RGB result
        improvement = mean_test_uar - rgb_baseline
        print(f'\nðŸŽ¯ Optical Flow Performance Improvement: +{improvement:.2f}% absolute')
        print(f'ðŸ“ˆ Expected UAR boost: +8-15% over RGB baseline')
        
        if mean_test_uar >= 45:
            print('ðŸ† CONSERVATIVE TARGET ACHIEVED!')
        if mean_test_uar >= 55:
            print('ðŸš€ REALISTIC TARGET ACHIEVED!')
        if mean_test_uar >= 65:
            print('ðŸŽª OPTIMISTIC TARGET ACHIEVED!')
    
    return results

def main():
    parser = argparse.ArgumentParser(description='ðŸš€ Optical Flow CNN - CASME-II Micro-Expression Recognition')
    parser.add_argument('--config', type=str, default='config/config.yaml', help='Path to config file')
    parser.add_argument('--device', type=str, default='auto', help='Device to use (auto/cuda/cpu)')
    args = parser.parse_args()
    
    # Load configuration
    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)
    
    # Set device
    if args.device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(args.device)
    
    print(f'ðŸš€ Using device: {device}')
    print(f'ðŸ“Š Configuration: {args.config}')
    print(f'ðŸ”¥ OPTICAL FLOW UPGRADE: Motion-based micro-expression recognition')
    
    # Print model info
    model = create_lightweight_model(num_classes=4)
    print(f'ðŸ§  Optical Flow CNN Model for CASME-II')
    print(f'ðŸ“ˆ Expected UAR Improvement: +8-15% over RGB baseline')
    
    # Create checkpoints directory
    Path('checkpoints').mkdir(exist_ok=True)
    
    # Run optical flow LOSO demo
    results = optical_flow_losos_demo(config, device)
    
    print(f'\nðŸŽ‰ Optical Flow LOSO Demo Complete!')
    print(f'ðŸš€ Your motion-based CNN with CASME-II research protocols is working!')
    print(f'ðŸ“Š Check results for UAR improvement with optical flow!')

if __name__ == '__main__':
    main()
